{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "\n",
    "This is our first implentation of a Neural Network. This notebook borrows heavily from [this](https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU) youtube series and [this](https://github.com/stephencwelch/Neural-Networks-Demystified) accompanying repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our inputs and outputs. `X` is a matrix with 2 features and 3 samples, `y` is a vector with 3 samples. Note that we normalize `y` so that its max value is `1.0`. This will become important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)\n",
    "\n",
    "# Normalize\n",
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100  # Max test score is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the Neural Network class! We won't go in depth on explaining everything going on in here because the youtube videos and git repo do so better than we could. One thing we did find really interesting that the video glossed over was that the sigmoid function is applied to the output layer. This makes it so that our network can only make predictions from 0-1. If you want your network to predict data that does not fall between 0 and 1, you have to use a slightly different architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        np.random.seed(0)  # seed the random number generator for consistent results\n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "\n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our neural network and make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yHat:\n",
      "[[ 0.67399915]\n",
      " [ 0.64461564]\n",
      " [ 0.66968583]]\n",
      "y:\n",
      "[[ 0.75]\n",
      " [ 0.82]\n",
      " [ 0.93]]\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "yHat = NN.forward(X)\n",
    "print 'yHat:'\n",
    "print yHat\n",
    "print 'y:'\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are some pretty terrible predictions. This makes sense though, since we basically randomly initialized our weight matrices and used them to make predictions. There's no reason for the predictions to be accurate. Let's update our weight matrices so that our network becomes better at making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trainer` is a class that uses `scipy`'s `optimize` module to update the weight matrices according to the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 89\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 96\n"
     ]
    }
   ],
   "source": [
    "T = trainer(NN)\n",
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yHat:\n",
      "[[ 0.74999632]\n",
      " [ 0.81999591]\n",
      " [ 0.92997684]]\n",
      "y:\n",
      "[[ 0.75]\n",
      " [ 0.82]\n",
      " [ 0.93]]\n"
     ]
    }
   ],
   "source": [
    "yHat = NN.forward(X)\n",
    "print 'yHat:'\n",
    "print yHat\n",
    "print 'y:'\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! These predictions are pretty accurate. In fact, they're *too* accurate. We have fallen victim to a common data scientist pitfall. Our network has learned to perfectly predict the data we trained it on, but it's probably not that good at predicting data it hasn't seen, since it just uses the exact pattern of the train set. There are lots of ways to address this, a common one is to add a [regularizer](https://en.wikipedia.org/wiki/Regularization_(mathematics)) to the cost function. We won't go through the implementation of this, as it's similar to regularizing a linear or logistic regression. You just use the sums of the weights inside the Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see what the optimizer is doing, here are some graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f7647d62790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEPCAYAAACDTflkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76680851d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(T.J)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, at each iteration the optimizer tweaks the weight matrices so that the network's predictions more accurately match `yHat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform a sanity-check to make sure that the gradients we compute using calculus are actually correct. We do so by computing an approximate numerical gradient, which we can do because of the [limit definition of a derivative](https://www.math.hmc.edu/calculus/tutorials/limit_definition/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(N, X, y):\n",
    "        paramsInitial = N.getParams()\n",
    "        numgrad = np.zeros(paramsInitial.shape)\n",
    "        perturb = np.zeros(paramsInitial.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(paramsInitial)):\n",
    "            #Set perturbation vector\n",
    "            perturb[p] = e\n",
    "            N.setParams(paramsInitial + perturb)\n",
    "            loss2 = N.costFunction(X, y)\n",
    "            \n",
    "            N.setParams(paramsInitial - perturb)\n",
    "            loss1 = N.costFunction(X, y)\n",
    "\n",
    "            #Compute Numerical Gradient\n",
    "            numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "\n",
    "            #Return the value we changed to zero:\n",
    "            perturb[p] = 0\n",
    "            \n",
    "        #Return Params to original value:\n",
    "        N.setParams(paramsInitial)\n",
    "\n",
    "        return numgrad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical gradient\n",
      "[  1.34904098e-07   1.97623342e-06  -6.07620253e-08   1.09120506e-07\n",
      "   1.76389396e-06  -6.53539629e-08  -2.55880567e-06  -5.86723938e-07\n",
      "  -2.77263500e-06]\n",
      "Actual gradient\n",
      "[  1.34904343e-07   1.97616103e-06  -6.07617572e-08   1.09121625e-07\n",
      "   1.76337727e-06  -6.53531423e-08  -2.55868640e-06  -5.86719246e-07\n",
      "  -2.77246933e-06]\n"
     ]
    }
   ],
   "source": [
    "numgrad = computeNumericalGradient(NN, X, y)\n",
    "grad = NN.computeGradients(X,y)\n",
    "print \"Numerical gradient\"\n",
    "print numgrad\n",
    "print \"Actual gradient\"\n",
    "print grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
