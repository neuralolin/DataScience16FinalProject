{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self, inputLayerSize=2, outputLayerSize=1, hiddenLayerSize=16, Lambda=0, alpha=0):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = inputLayerSize\n",
    "        self.outputLayerSize = outputLayerSize\n",
    "        self.hiddenLayerSize = hiddenLayerSize\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.random((self.inputLayerSize,self.hiddenLayerSize))*2 - 1\n",
    "        self.W2 = np.random.random((self.hiddenLayerSize,self.outputLayerSize))*2 - 1\n",
    "        \n",
    "        self.W_h = 2*np.random.random((self.hiddenLayerSize,self.hiddenLayerSize)) - 1\n",
    "        \n",
    "        self.Lambda = Lambda\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "    def forward(self, X, prev_hidden=None):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        if prev_hidden is not None:\n",
    "            self.z2 += np.dot(prev_hidden, self.W_h)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        self.a3 = self.sigmoid(self.z3) \n",
    "        return self.a3\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    # convert output of sigmoid function to its derivative\n",
    "    def sigmoid_output_to_derivative(self, output):\n",
    "        return output*(1-output)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        J += (self.Lambda/2)*(sum(self.W1**2)+sum(self.W2**2))\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3) + self.Lambda*self.W2\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2) + self.Lambda*self.W1\n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "np.random.seed(0)\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "# this is just a dictionary mapping from ints to their binary representation.\n",
    "# makes conversion later easier\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "nn = Neural_Network(inputLayerSize=input_dim, outputLayerSize=output_dim, hiddenLayerSize=hidden_dim, Lambda=0, alpha=0.1)\n",
    "\n",
    "W1_update = np.zeros_like(nn.W1)\n",
    "W2_update = np.zeros_like(nn.W2)\n",
    "W_h_update = np.zeros_like(nn.W_h)\n",
    "\n",
    "# training logic. Train our RNN on 10,000 addition problems\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    output_layer_deltas = []\n",
    "    hidden_layer_values = []\n",
    "    # assume the hidden layer was zero to begin with so our NN can reference\n",
    "    # the previous hidden layer\n",
    "    hidden_layer_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # FORWARD PROP:\n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],\n",
    "                     b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "        \n",
    "        prediction = nn.forward(X, prev_hidden=hidden_layer_values[-1])\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        output_error = y - prediction\n",
    "        # this is the backpropagating error, represented by a delta.\n",
    "        output_layer_deltas.append((output_error)*nn.sigmoidPrime(nn.z3))\n",
    "        overallError += np.abs(output_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(prediction[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        hidden_layer_values.append(copy.deepcopy(nn.a2))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    # BACKWARD PROP:\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        hidden_layer = hidden_layer_values[-position-1]\n",
    "        prev_hidden_layer = hidden_layer_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = output_layer_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(\n",
    "                             nn.W_h.T) +layer_2_delta.dot(\n",
    "                             nn.W2.T)) * nn.sigmoid_output_to_derivative(\n",
    "        hidden_layer)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        W2_update += np.atleast_2d(hidden_layer).T.dot(layer_2_delta)\n",
    "        W_h_update += np.atleast_2d(prev_hidden_layer).T.dot(layer_1_delta)\n",
    "        W1_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "\n",
    "    nn.W1 += W1_update * alpha\n",
    "    nn.W2 += W2_update * alpha\n",
    "    nn.W_h += W_h_update * alpha\n",
    "    \n",
    "    W1_update *= 0\n",
    "    W2_update *= 0\n",
    "    W_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print \"Error:\" + str(overallError)\n",
    "        print \"Pred:\" + str(d)\n",
    "        print \"True:\" + str(c)\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print str(a_int) + \" + \" + str(b_int) + \" = \" + str(out)\n",
    "        print \"------------\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
